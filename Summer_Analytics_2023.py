# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l4dJiOEvIO2crKpmITlkqafhriINBMSM
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score

# Step 1: Load the training and testing dataset
train_data = pd.read_csv('/Train_Data.csv')
test_data = pd.read_csv('/Test_Data.csv')
test_data = test_data.drop(test_data.columns[0], axis=1)

# Step 2: Perform exploratory data analysis (EDA)
# Check for missing values
print(train_data.isnull().sum())
print(test_data.isnull().sum())

# Analyze distributions and summary statistics
print(train_data.describe())
print(test_data.describe())

# Step 3: Perform feature engineering
# Encode categorical variables
train_data = pd.get_dummies(train_data, columns=['pc', 'ma'])
test_data = pd.get_dummies(test_data, columns=['pc', 'ma'])
# Scale numerical features
scaler = StandardScaler()
train_data[['ld', 'm0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']] = scaler.fit_transform(train_data[['ld', 'm0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']])
scaler = StandardScaler()
test_data[['ld', 'm0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']] = scaler.fit_transform(test_data[['ld', 'm0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']])
# Create new features through transformations or combinations
train_data['m_sum'] = train_data[['m0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']].sum(axis=1)
train_data['m_mean'] = train_data[['m0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']].mean(axis=1)
test_data['m_sum'] = test_data[['m0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']].sum(axis=1)
test_data['m_mean'] = test_data[['m0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']].mean(axis=1)
# Step 4: Split the training data into features and target variable
X = train_data.drop(['pred'], axis=1)
Z_test = test_data
y = train_data['pred']

# Step 5: Split the Training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Feature scaling (if required)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
Z_test_scaled = scaler.transform(Z_test)
# Step 7: Handle missing values
imputer = SimpleImputer(strategy='mean')
X_train_scaled = imputer.fit_transform(X_train_scaled)
X_val_scaled = imputer.transform(X_val_scaled)
Z_test_scaled = imputer.transform(Z_test_scaled)
# Step 8: Train the model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Step 9: Make predictions on the validation  and testing set
y_pred = model.predict(X_val_scaled)
Z_pred=  model.predict(Z_test_scaled)
# Step 10: Evaluate the model's performance
accuracy = accuracy_score(y_val, y_pred)
print(f'Validation accuracy: {accuracy:.4f}')

predictions_df = pd.DataFrame({'pred': Z_pred})

# Step 17: Save predictions to a new CSV file
predictions_df.to_csv('/content/test_predictions.csv', index=False)